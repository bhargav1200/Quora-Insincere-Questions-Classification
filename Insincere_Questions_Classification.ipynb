{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bhargav/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import f1_score\n",
    "from torch import optim\n",
    "import torchtext\n",
    "import random\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/999994 [00:00<?, ?it/s]Skipping token b'999994' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|█████████▉| 999510/999994 [01:33<00:00, 10740.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([85065, 300])\n"
     ]
    }
   ],
   "source": [
    "text = torchtext.data.Field(lower=True, batch_first=True, tokenize=word_tokenize)\n",
    "qid = torchtext.data.Field()\n",
    "target = torchtext.data.Field(sequential=False, use_vocab=False, is_target=True)\n",
    "train = torchtext.data.TabularDataset(path='data/train.csv', format='csv',\n",
    "                                      fields={'question_text': ('text',text),\n",
    "                                              'target': ('target',target)})\n",
    "test = torchtext.data.TabularDataset(path='data/test.csv', format='csv',\n",
    "                                     fields={'qid': ('qid', qid),\n",
    "                                             'question_text': ('text', text)})\n",
    "text.build_vocab(train, test, min_freq=3)\n",
    "qid.build_vocab(test)\n",
    "text.vocab.load_vectors(torchtext.vocab.Vectors('data/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'))\n",
    "print(text.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2018)\n",
    "train, val = train.split(split_ratio=0.9, random_state=random.getstate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████▉| 999510/999994 [01:50<00:00, 10740.29it/s]"
     ]
    }
   ],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, pretrained_lm, padding_idx, static=True, hidden_dim=128, lstm_layer=2, dropout=0.2):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_lm)\n",
    "        self.embedding.padding_idx = padding_idx\n",
    "        if static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding.embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=lstm_layer, \n",
    "                            dropout = dropout,\n",
    "                            bidirectional=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim*lstm_layer*2, 1)\n",
    "    \n",
    "    def forward(self, sents):\n",
    "        x = self.embedding(sents)\n",
    "        x = torch.transpose(x, dim0=1, dim1=0)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        y = self.hidden2label(self.dropout(torch.cat([c_n[i,:, :] for i in range(c_n.shape[0])], dim=1)))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def training(epoch, model, eval_every, loss_func, optimizer, train_iter, val_iter, early_stop=1, warmup_epoch=2):\n",
    "    \n",
    "    step = 0\n",
    "    max_loss = 1e5\n",
    "    no_improve_epoch = 0\n",
    "    no_improve_in_previous_epoch = False\n",
    "    fine_tuning = False\n",
    "    train_record = []\n",
    "    val_record = []\n",
    "    losses = []\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        if e >= warmup_epoch:\n",
    "            if no_improve_in_previous_epoch:\n",
    "                no_improve_epoch += 1\n",
    "                if no_improve_epoch >= early_stop:\n",
    "                    break\n",
    "            else:\n",
    "                no_improve_epoch = 0\n",
    "            no_improve_in_previous_epoch = True\n",
    "        if not fine_tuning and e >= warmup_epoch:\n",
    "            model.embedding.weight.requires_grad = True\n",
    "            fine_tuning = True\n",
    "        train_iter.init_epoch()\n",
    "        for train_batch in iter(train_iter):\n",
    "            step += 1\n",
    "            model.train()\n",
    "            x = train_batch.text.cuda()\n",
    "            y = train_batch.target.type(torch.Tensor).cuda()\n",
    "            model.zero_grad()\n",
    "            pred = model.forward(x).view(-1)\n",
    "            loss = loss_function(pred, y)\n",
    "            losses.append(loss.cpu().data.numpy())\n",
    "            train_record.append(loss.cpu().data.numpy())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if step % eval_every == 0:\n",
    "                model.eval()\n",
    "                model.zero_grad()\n",
    "                val_loss = []\n",
    "                for val_batch in iter(val_iter):\n",
    "                    val_x = val_batch.text.cuda()\n",
    "                    val_y = val_batch.target.type(torch.Tensor).cuda()\n",
    "                    val_pred = model.forward(val_x).view(-1)\n",
    "                    val_loss.append(loss_function(val_pred, val_y).cpu().data.numpy())\n",
    "                val_record.append({'step': step, 'loss': np.mean(val_loss)})\n",
    "                print('epcoh {:02} - step {:06} - train_loss {:.4f} - val_loss {:.4f} '.format(\n",
    "                            e, step, np.mean(losses), val_record[-1]['loss']))\n",
    "                if e >= warmup_epoch:\n",
    "                    if val_record[-1]['loss'] <= max_loss:\n",
    "                        save(m=model, info={'step': step, 'epoch': e, 'train_loss': np.mean(losses),\n",
    "                                            'val_loss': val_record[-1]['loss']})\n",
    "                        max_loss = val_record[-1]['loss']\n",
    "                        no_improve_in_previous_epoch = False\n",
    "    \n",
    "\n",
    "def save(m, info):\n",
    "    torch.save(info, 'best_model.info')\n",
    "    torch.save(m, 'best_model.m')\n",
    "    \n",
    "def load():\n",
    "    m = torch.load('best_model.m')\n",
    "    info = torch.load('best_model.info')\n",
    "    return m, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_iter = torchtext.data.BucketIterator(dataset=train,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sort_key=lambda x: x.text.__len__(),\n",
    "                                               shuffle=True,\n",
    "                                               sort=False)\n",
    "val_iter = torchtext.data.BucketIterator(dataset=val,\n",
    "                                             batch_size=batch_size,\n",
    "                                             sort_key=lambda x: x.text.__len__(),\n",
    "                                             train=False,\n",
    "                                             sort=False)\n",
    "model = BiLSTM(text.vocab.vectors, lstm_layer=2, padding_idx=text.vocab.stoi[text.pad_token], hidden_dim=128).cuda()\n",
    "# loss_function = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([pos_w]).cuda())\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                    lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoh 00 - step 000500 - train_loss 0.1487 - val_loss 0.1268 \n",
      "epcoh 00 - step 001000 - train_loss 0.1366 - val_loss 0.1267 \n",
      "epcoh 00 - step 001500 - train_loss 0.1330 - val_loss 0.1217 \n",
      "epcoh 00 - step 002000 - train_loss 0.1301 - val_loss 0.1222 \n",
      "epcoh 00 - step 002500 - train_loss 0.1282 - val_loss 0.1181 \n",
      "epcoh 00 - step 003000 - train_loss 0.1265 - val_loss 0.1154 \n",
      "epcoh 00 - step 003500 - train_loss 0.1249 - val_loss 0.1139 \n",
      "epcoh 00 - step 004000 - train_loss 0.1240 - val_loss 0.1124 \n",
      "epcoh 00 - step 004500 - train_loss 0.1230 - val_loss 0.1116 \n",
      "epcoh 00 - step 005000 - train_loss 0.1219 - val_loss 0.1115 \n",
      "epcoh 00 - step 005500 - train_loss 0.1209 - val_loss 0.1108 \n",
      "epcoh 00 - step 006000 - train_loss 0.1199 - val_loss 0.1113 \n",
      "epcoh 00 - step 006500 - train_loss 0.1194 - val_loss 0.1093 \n",
      "epcoh 00 - step 007000 - train_loss 0.1186 - val_loss 0.1096 \n",
      "epcoh 00 - step 007500 - train_loss 0.1180 - val_loss 0.1080 \n",
      "epcoh 00 - step 008000 - train_loss 0.1174 - val_loss 0.1088 \n",
      "epcoh 00 - step 008500 - train_loss 0.1171 - val_loss 0.1091 \n",
      "epcoh 00 - step 009000 - train_loss 0.1167 - val_loss 0.1086 \n",
      "epcoh 01 - step 009500 - train_loss 0.1160 - val_loss 0.1060 \n",
      "epcoh 01 - step 010000 - train_loss 0.1156 - val_loss 0.1077 \n",
      "epcoh 01 - step 010500 - train_loss 0.1150 - val_loss 0.1065 \n",
      "epcoh 01 - step 011000 - train_loss 0.1146 - val_loss 0.1099 \n",
      "epcoh 01 - step 011500 - train_loss 0.1142 - val_loss 0.1072 \n",
      "epcoh 01 - step 012000 - train_loss 0.1137 - val_loss 0.1055 \n",
      "epcoh 01 - step 012500 - train_loss 0.1133 - val_loss 0.1053 \n",
      "epcoh 01 - step 013000 - train_loss 0.1130 - val_loss 0.1078 \n",
      "epcoh 01 - step 013500 - train_loss 0.1127 - val_loss 0.1064 \n",
      "epcoh 01 - step 014000 - train_loss 0.1125 - val_loss 0.1065 \n",
      "epcoh 01 - step 014500 - train_loss 0.1122 - val_loss 0.1049 \n",
      "epcoh 01 - step 015000 - train_loss 0.1119 - val_loss 0.1049 \n",
      "epcoh 01 - step 015500 - train_loss 0.1116 - val_loss 0.1079 \n",
      "epcoh 01 - step 016000 - train_loss 0.1114 - val_loss 0.1044 \n",
      "epcoh 01 - step 016500 - train_loss 0.1111 - val_loss 0.1031 \n",
      "epcoh 01 - step 017000 - train_loss 0.1109 - val_loss 0.1077 \n",
      "epcoh 01 - step 017500 - train_loss 0.1107 - val_loss 0.1054 \n",
      "epcoh 01 - step 018000 - train_loss 0.1104 - val_loss 0.1030 \n",
      "epcoh 02 - step 018500 - train_loss 0.1102 - val_loss 0.1015 \n",
      "epcoh 02 - step 019000 - train_loss 0.1099 - val_loss 0.1028 \n",
      "epcoh 02 - step 019500 - train_loss 0.1097 - val_loss 0.1030 \n",
      "epcoh 02 - step 020000 - train_loss 0.1094 - val_loss 0.1045 \n",
      "epcoh 02 - step 020500 - train_loss 0.1091 - val_loss 0.1036 \n",
      "epcoh 02 - step 021000 - train_loss 0.1089 - val_loss 0.1015 \n",
      "epcoh 02 - step 021500 - train_loss 0.1087 - val_loss 0.1026 \n",
      "epcoh 02 - step 022000 - train_loss 0.1084 - val_loss 0.1021 \n",
      "epcoh 02 - step 022500 - train_loss 0.1082 - val_loss 0.1041 \n",
      "epcoh 02 - step 023000 - train_loss 0.1079 - val_loss 0.1011 \n",
      "epcoh 02 - step 023500 - train_loss 0.1077 - val_loss 0.1016 \n",
      "epcoh 02 - step 024000 - train_loss 0.1075 - val_loss 0.1016 \n",
      "epcoh 02 - step 024500 - train_loss 0.1073 - val_loss 0.1039 \n",
      "epcoh 02 - step 025000 - train_loss 0.1071 - val_loss 0.1013 \n",
      "epcoh 02 - step 025500 - train_loss 0.1069 - val_loss 0.1041 \n",
      "epcoh 02 - step 026000 - train_loss 0.1068 - val_loss 0.1009 \n",
      "epcoh 02 - step 026500 - train_loss 0.1066 - val_loss 0.1010 \n",
      "epcoh 02 - step 027000 - train_loss 0.1065 - val_loss 0.1017 \n",
      "epcoh 02 - step 027500 - train_loss 0.1063 - val_loss 0.1008 \n",
      "epcoh 03 - step 028000 - train_loss 0.1060 - val_loss 0.1033 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhargav/anaconda3/envs/detection_torch/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type BiLSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoh 03 - step 028500 - train_loss 0.1058 - val_loss 0.1015 \n",
      "epcoh 03 - step 029000 - train_loss 0.1057 - val_loss 0.1009 \n",
      "epcoh 03 - step 029500 - train_loss 0.1055 - val_loss 0.1025 \n",
      "epcoh 03 - step 030000 - train_loss 0.1053 - val_loss 0.1012 \n",
      "epcoh 03 - step 030500 - train_loss 0.1051 - val_loss 0.1023 \n",
      "epcoh 03 - step 031000 - train_loss 0.1049 - val_loss 0.1012 \n",
      "epcoh 03 - step 031500 - train_loss 0.1047 - val_loss 0.1005 \n",
      "epcoh 03 - step 032000 - train_loss 0.1046 - val_loss 0.1032 \n",
      "epcoh 03 - step 032500 - train_loss 0.1044 - val_loss 0.1003 \n",
      "epcoh 03 - step 033000 - train_loss 0.1042 - val_loss 0.1015 \n",
      "epcoh 03 - step 033500 - train_loss 0.1041 - val_loss 0.1003 \n",
      "epcoh 03 - step 034000 - train_loss 0.1039 - val_loss 0.1016 \n",
      "epcoh 03 - step 034500 - train_loss 0.1038 - val_loss 0.1005 \n",
      "epcoh 03 - step 035000 - train_loss 0.1037 - val_loss 0.1003 \n",
      "epcoh 03 - step 035500 - train_loss 0.1035 - val_loss 0.1021 \n",
      "epcoh 03 - step 036000 - train_loss 0.1034 - val_loss 0.0993 \n",
      "epcoh 03 - step 036500 - train_loss 0.1032 - val_loss 0.1002 \n",
      "epcoh 04 - step 037000 - train_loss 0.1031 - val_loss 0.1029 \n",
      "epcoh 04 - step 037500 - train_loss 0.1028 - val_loss 0.1002 \n",
      "epcoh 04 - step 038000 - train_loss 0.1027 - val_loss 0.1027 \n",
      "epcoh 04 - step 038500 - train_loss 0.1025 - val_loss 0.1017 \n",
      "epcoh 04 - step 039000 - train_loss 0.1023 - val_loss 0.1025 \n",
      "epcoh 04 - step 039500 - train_loss 0.1021 - val_loss 0.1012 \n",
      "epcoh 04 - step 040000 - train_loss 0.1020 - val_loss 0.0999 \n",
      "epcoh 04 - step 040500 - train_loss 0.1018 - val_loss 0.1000 \n",
      "epcoh 04 - step 041000 - train_loss 0.1016 - val_loss 0.1011 \n",
      "epcoh 04 - step 041500 - train_loss 0.1015 - val_loss 0.1023 \n",
      "epcoh 04 - step 042000 - train_loss 0.1013 - val_loss 0.1000 \n",
      "epcoh 04 - step 042500 - train_loss 0.1012 - val_loss 0.1004 \n",
      "epcoh 04 - step 043000 - train_loss 0.1011 - val_loss 0.0997 \n",
      "epcoh 04 - step 043500 - train_loss 0.1009 - val_loss 0.0996 \n",
      "epcoh 04 - step 044000 - train_loss 0.1008 - val_loss 0.0991 \n",
      "epcoh 04 - step 044500 - train_loss 0.1007 - val_loss 0.0993 \n",
      "epcoh 04 - step 045000 - train_loss 0.1006 - val_loss 0.1013 \n",
      "epcoh 04 - step 045500 - train_loss 0.1004 - val_loss 0.1002 \n",
      "epcoh 05 - step 046000 - train_loss 0.1003 - val_loss 0.1009 \n",
      "epcoh 05 - step 046500 - train_loss 0.1001 - val_loss 0.1018 \n",
      "epcoh 05 - step 047000 - train_loss 0.0999 - val_loss 0.1032 \n",
      "epcoh 05 - step 047500 - train_loss 0.0998 - val_loss 0.1011 \n",
      "epcoh 05 - step 048000 - train_loss 0.0996 - val_loss 0.1021 \n",
      "epcoh 05 - step 048500 - train_loss 0.0994 - val_loss 0.1005 \n",
      "epcoh 05 - step 049000 - train_loss 0.0992 - val_loss 0.1028 \n",
      "epcoh 05 - step 049500 - train_loss 0.0991 - val_loss 0.1007 \n",
      "epcoh 05 - step 050000 - train_loss 0.0989 - val_loss 0.1025 \n",
      "epcoh 05 - step 050500 - train_loss 0.0988 - val_loss 0.1026 \n",
      "epcoh 05 - step 051000 - train_loss 0.0986 - val_loss 0.1038 \n",
      "epcoh 05 - step 051500 - train_loss 0.0985 - val_loss 0.1056 \n",
      "epcoh 05 - step 052000 - train_loss 0.0984 - val_loss 0.1016 \n",
      "epcoh 05 - step 052500 - train_loss 0.0982 - val_loss 0.1006 \n",
      "epcoh 05 - step 053000 - train_loss 0.0981 - val_loss 0.1009 \n",
      "epcoh 05 - step 053500 - train_loss 0.0979 - val_loss 0.1034 \n",
      "epcoh 05 - step 054000 - train_loss 0.0978 - val_loss 0.1007 \n",
      "epcoh 05 - step 054500 - train_loss 0.0977 - val_loss 0.1019 \n",
      "epcoh 05 - step 055000 - train_loss 0.0976 - val_loss 0.1012 \n",
      "epcoh 06 - step 055500 - train_loss 0.0974 - val_loss 0.1083 \n",
      "epcoh 06 - step 056000 - train_loss 0.0972 - val_loss 0.1076 \n",
      "epcoh 06 - step 056500 - train_loss 0.0970 - val_loss 0.1066 \n",
      "epcoh 06 - step 057000 - train_loss 0.0968 - val_loss 0.1091 \n",
      "epcoh 06 - step 057500 - train_loss 0.0967 - val_loss 0.1060 \n",
      "epcoh 06 - step 058000 - train_loss 0.0965 - val_loss 0.1056 \n",
      "epcoh 06 - step 058500 - train_loss 0.0963 - val_loss 0.1046 \n",
      "epcoh 06 - step 059000 - train_loss 0.0962 - val_loss 0.1073 \n",
      "epcoh 06 - step 059500 - train_loss 0.0960 - val_loss 0.1053 \n",
      "epcoh 06 - step 060000 - train_loss 0.0959 - val_loss 0.1104 \n",
      "epcoh 06 - step 060500 - train_loss 0.0957 - val_loss 0.1096 \n",
      "epcoh 06 - step 061000 - train_loss 0.0956 - val_loss 0.1032 \n",
      "epcoh 06 - step 061500 - train_loss 0.0955 - val_loss 0.1083 \n",
      "epcoh 06 - step 062000 - train_loss 0.0954 - val_loss 0.1057 \n",
      "epcoh 06 - step 062500 - train_loss 0.0952 - val_loss 0.1071 \n",
      "epcoh 06 - step 063000 - train_loss 0.0951 - val_loss 0.1106 \n",
      "epcoh 06 - step 063500 - train_loss 0.0950 - val_loss 0.1027 \n",
      "epcoh 06 - step 064000 - train_loss 0.0948 - val_loss 0.1050 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "training(model=model, epoch=20, eval_every=500,\n",
    "         loss_func=loss_function, optimizer=optimizer, train_iter=train_iter,\n",
    "        val_iter=val_iter, warmup_epoch=3, early_stop=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step': 44000, 'epoch': 4, 'train_loss': 0.10083235, 'val_loss': 0.09905402}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, m_info = load()\n",
    "m_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lstm.flatten_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold is 0.3300 with F1 score: 0.6803\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.eval()\n",
    "val_pred = []\n",
    "val_true = []\n",
    "val_iter.init_epoch()\n",
    "for val_batch in iter(val_iter):\n",
    "    val_x = val_batch.text.cuda()\n",
    "    val_true += val_batch.target.data.numpy().tolist()\n",
    "    val_pred += torch.sigmoid(model.forward(val_x).view(-1)).cpu().data.numpy().tolist()\n",
    "\n",
    "\n",
    "\n",
    "tmp = [0,0,0] # idx, cur, max\n",
    "delta = 0\n",
    "for tmp[0] in np.arange(0.1, 0.501, 0.01):\n",
    "    tmp[1] = f1_score(val_true, np.array(val_pred)>tmp[0])\n",
    "    if tmp[1] > tmp[2]:\n",
    "        delta = tmp[0]\n",
    "        tmp[2] = tmp[1]\n",
    "print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "test_iter = torchtext.data.BucketIterator(dataset=test,\n",
    "                                    batch_size=batch_size,\n",
    "                                    sort_key=lambda x: x.text.__len__(),\n",
    "                                    sort=True)\n",
    "test_pred = []\n",
    "test_id = []\n",
    "\n",
    "for test_batch in iter(test_iter):\n",
    "    test_x = test_batch.text.cuda()\n",
    "    test_pred += torch.sigmoid(model.forward(test_x).view(-1)).cpu().data.numpy().tolist()\n",
    "    test_id += test_batch.qid.view(-1).data.numpy().tolist()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df =pd.DataFrame()\n",
    "sub_df['qid'] = [qid.vocab.itos[i] for i in test_id]\n",
    "sub_df['prediction'] = (np.array(test_pred) >= delta).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
